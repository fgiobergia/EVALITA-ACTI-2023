{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05aeacdf-9a93-4302-a48d-8230d825494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88a38ae9-327e-4b25-9ed0-e51769ee8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((10, 5))\n",
    "labels = torch.randint(0,6,size=(10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "592bfe87-b628-400d-8f06-a711906b9169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 3, 5, 5, 4, 3, 0, 1, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b460293c-3dfa-4f3a-a57e-d6892c0803a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0525, 0.4710, 0.6609, 0.9020, 0.8741],\n",
       "        [0.4473, 0.3561, 0.1850, 0.6548, 0.3493],\n",
       "        [0.6517, 0.4642, 0.9187, 0.5817, 0.8642],\n",
       "        [0.4555, 0.3273, 0.6956, 0.2482, 0.9972],\n",
       "        [0.0563, 0.9447, 0.4498, 0.7288, 0.9153],\n",
       "        [0.5834, 0.0771, 0.7608, 0.2570, 0.3692],\n",
       "        [0.8934, 0.5532, 0.3962, 0.3744, 0.1055],\n",
       "        [0.2896, 0.4702, 0.7483, 0.8952, 0.6533],\n",
       "        [0.3158, 0.6109, 0.9765, 0.1819, 0.2682],\n",
       "        [0.0892, 0.8559, 0.2724, 0.4401, 0.1413]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73158241-8925-4592-b7ad-c9d2ed677e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5887d127-a9da-4d27-aff9-876d4d3fdf64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b5b78a2-7f8b-42e2-b65f-e7c7a7c98b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_known = torch.nn.CrossEntropyLoss()(x[labels != 5], labels[labels!=5])\n",
    "loss_unknown = torch.nn.CrossEntropyLoss()(x[labels==5], x[labels==5, :4].argmax(axis=1))\n",
    "\n",
    "lmbda = 1.\n",
    "# loss = self.loss_binary(x_binary.flatten(), labels[:, 0].float()) + lmbda * self.loss_categ(x_categ[~zero_cat], labels[~zero_cat, 1])\n",
    "\n",
    "loss = loss_known + lmbda * loss_unknown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07b0bc3c-1922-44e0-82cf-4ebb5df05e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0525, 0.4710, 0.6609, 0.9020, 0.8741],\n",
       "         [0.4473, 0.3561, 0.1850, 0.6548, 0.3493],\n",
       "         [0.6517, 0.4642, 0.9187, 0.5817, 0.8642],\n",
       "         [0.5834, 0.0771, 0.7608, 0.2570, 0.3692],\n",
       "         [0.8934, 0.5532, 0.3962, 0.3744, 0.1055],\n",
       "         [0.2896, 0.4702, 0.7483, 0.8952, 0.6533],\n",
       "         [0.3158, 0.6109, 0.9765, 0.1819, 0.2682],\n",
       "         [0.0892, 0.8559, 0.2724, 0.4401, 0.1413]]),\n",
       " tensor([2, 3, 3, 4, 3, 0, 1, 2]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[labels != 5], labels[labels!=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a43f72ba-3e05-4e05-b89f-2001d97e995f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4555, 0.3273, 0.6956, 0.2482, 0.9972],\n",
       "         [0.0563, 0.9447, 0.4498, 0.7288, 0.9153]]),\n",
       " tensor([2, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[labels==5], x[labels==5, :4].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53665b34-713c-497f-9c19-bef28b4fbf01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.22.4)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Using cached huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dauin_user/f.giobergia/.local/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.5.1)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (2022.5.0)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Using cached pyarrow-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.4.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.10)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: tokenizers, xxhash, pyarrow, multidict, frozenlist, filelock, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets, evaluate\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.5.1\n",
      "    Uninstalling dill-0.3.5.1:\n",
      "      Successfully uninstalled dill-0.3.5.1\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 evaluate-0.4.0 filelock-3.12.0 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 pyarrow-12.0.0 responses-0.18.0 tokenizers-0.13.3 transformers-4.28.1 xxhash-3.2.0 yarl-1.9.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cbd95c4-38ab-44c7-a219-1e1e7cbf1072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37dd3b51-b47a-4357-85f8-8b08c77ba394",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = pd.read_csv(\"subtaskA_train.csv\", index_col=0)\n",
    "df_test_A = pd.read_csv(\"subtaskA_test.csv\", index_col=0)\n",
    "\n",
    "df_B = pd.read_csv(\"subtaskB_train.csv\", index_col=0).drop(columns=[\"topic\"])\n",
    "df_test_B = pd.read_csv(\"subtaskB_test.csv\", index_col=0)\n",
    "\n",
    "df = df_A.merge(df_B, on=\"comment_text\", how=\"outer\")\n",
    "\n",
    "df.loc[~df[\"conspiracy\"].isna(), \"conspiratorial\"] = 1\n",
    "df.loc[df[\"conspiratorial\"] == 0, \"conspiracy\"] = -1\n",
    "df.loc[df[\"conspiracy\"].isna(), \"conspiracy\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a4e299a-c746-45ab-a6c2-c399668abff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f3454c1-ddac-4d18-a365-112f28b290a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1aae981-8a14-49d1-901d-3bedd214226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f8329b-c0c6-4ddd-9652-32d439028d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"morenolq/bart-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b350efa9-b5a6-4465-aff4-edbe79c191d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c4fb2e9-0ca2-4db2-83c8-e771a6b6831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c43f6a45-74a3-4233-8ae2-b6343a6f6bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfidf import load_vocabs, get_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c11e2f09-cd14-4357-9c82-0c94c2d47f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs_tfidf, mat_tfidf = load_vocabs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba6c17-9a20-4f97-ba71-8a6af03f798f",
   "metadata": {},
   "source": [
    "### Encode with transformer's tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ddd16d2e-ae67-45ce-b942-0f398f21fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer(df.comment_text.tolist(), max_length=200, padding=True, return_tensors=\"pt\", truncation=True)[\"input_ids\"].cuda()\n",
    "n_tokens = 52000\n",
    "# X = X[y[:,1] != -1]\n",
    "# y = y[y[:,1] != -1]\n",
    "\n",
    "# X_test = tokenizer(df_test.comment_text.tolist(), max_length=200, padding=True, return_tensors=\"pt\", truncation=True)[\"input_ids\"]\n",
    "# y_test = torch.tensor(df_test.conspiratorial.values).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5df15c-2ab4-4399-afb9-2b9edfe6206b",
   "metadata": {},
   "source": [
    "### Encode with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d1503177-2caa-4d54-aef7-9aaac806a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from unidecode import unidecode\n",
    "\n",
    "def encode_df(df, tokens_map, max_len = None):\n",
    "    tokenized = []\n",
    "    for sent in df.comment_text:\n",
    "        toks = tokenizer.tokenize(unidecode(sent).lower())\n",
    "        tokenized.append([ tokens_map.get(tok, len(tokens_map)) for tok in toks ])\n",
    "    \n",
    "    if max_len is not None:\n",
    "        max_len = max_len if max_len != \"compute\" else max(map(len, tokenized))\n",
    "        tokenized = [ tokens[:max_len] + [len(tokens_map)] * (max_len - len(tokens)) for tokens in tokenized ]\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "tok_freq = {}\n",
    "for sent in df.comment_text:\n",
    "    toks = tokenizer.tokenize(unidecode(sent).lower())\n",
    "    for tok in toks:\n",
    "        tok_freq[tok] = tok_freq.get(tok, 0) + 1\n",
    "        \n",
    "threshold = 0# 0 => take all!\n",
    "# tokens = { k: v for k, v in tok_freq.items() if v > threshold }\n",
    "tokens = sorted([ k for k, v in tok_freq.items() if v > threshold ])\n",
    "tokens_map = { k: v for v, k in enumerate(tokens) }\n",
    "\n",
    "n_tokens = len(tokens_map)\n",
    "\n",
    "X = torch.tensor(encode_df(df, tokens_map, max_len=\"compute\")).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ae49e7fd-0c3e-47a6-983b-cffda0339bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(df[[\"conspiratorial\", \"conspiracy\"]].values).float().cuda()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c223dedd-7ae6-42c4-a91f-e22431456fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3c31e2c2-046b-4839-a00e-e2c1f8e96b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_info = np.vstack([ get_vec(t, vocabs_tfidf, mat_tfidf) for t in tokens ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "230b56e1-6596-4db5-b54b-90b73c558398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, n_tokens=52000, extra_vecs=None):\n",
    "        super().__init__()\n",
    "        emb_size = 768\n",
    "        conv1_channels = 64\n",
    "        proj_size = 16\n",
    "        \n",
    "        if n_tokens == 52000:\n",
    "            # assuming bart-it used, use padding_idx = 1 (filler token)\n",
    "            padding_idx = 1\n",
    "        else:\n",
    "            # assuming custom tokenizer, use padding_idx = n_tokens + 1 for the filler\n",
    "            padding_idx = n_tokens\n",
    "            n_tokens += 1\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings=n_tokens, embedding_dim=emb_size, padding_idx=padding_idx)\n",
    "        \n",
    "        if extra_vecs is not None:\n",
    "            # make sure we are passing a matrix shaped correctly\n",
    "            # (+1 to account for the +1 that is being passed when n_tokens != 52000\n",
    "            # NOTE: we do not expect to use this stuff when using pretrained vectors!\n",
    "            # (b/c they tokenize things weirdly and it's hard to have a 1:1 mapping w/ tfidf\n",
    "            assert n_tokens == extra_vecs.shape[0] + 1\n",
    "            self.extra_emb = nn.Embedding(num_embeddings=n_tokens, embedding_dim=extra_vecs.shape[1], padding_idx=padding_idx)\n",
    "            with torch.no_grad():\n",
    "                self.extra_emb.weight[:-1] = torch.tensor(extra_vecs)\n",
    "            \n",
    "            emb_size += extra_vecs.shape[1]\n",
    "        else:\n",
    "            self.extra_emb = None\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(emb_size, conv1_channels, stride=1, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(conv1_channels, 32, stride=1, kernel_size=5)\n",
    "        self.conv3 = nn.Conv1d(32, 8, stride=3, kernel_size=5)\n",
    "        \n",
    "        self.mp = nn.MaxPool1d(kernel_size=3, stride=3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(56, proj_size)\n",
    "        self.head_bin = nn.Linear(proj_size, 1)\n",
    "        self.head_cat = nn.Linear(proj_size, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.emb(x).permute(0, 2, 1)\n",
    "        if self.extra_emb is not None:\n",
    "            x = torch.cat([ x1, self.extra_emb(x).permute(0,2,1) ], axis=1)\n",
    "        else:\n",
    "            x = x1\n",
    "        x = torch.relu(self.mp(self.conv1(x)))\n",
    "        x = torch.relu(self.mp(self.conv2(x)))\n",
    "        x = torch.relu(self.mp(self.conv3(x)))\n",
    "        \n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x_bin = self.head_bin(x)\n",
    "        x_cat = self.head_cat(x)\n",
    "        \n",
    "        return x_bin.flatten(), x_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "826f821a-c71e-42b5-8571-cf9351c5ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "068dfe15-ba37-4bad-a987-eeff152dc1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_xent():\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    xent = nn.CrossEntropyLoss()\n",
    "    mse = nn.MSELoss()\n",
    "    \n",
    "    def loss_fct(y_bin_pred, y_cat_pred, y_true):\n",
    "        \n",
    "        # bce to be computed on all samples\n",
    "        loss_bin = bce(y_bin_pred, y_true[:,0])\n",
    "        \n",
    "        # cross-entropy only computed on samples for which a category is available (!= -1)\n",
    "        mask = y_true[:, 1] != -1\n",
    "        \n",
    "        loss_xent = xent(y_cat_pred[mask], y_true[mask, 1].long())\n",
    "        \n",
    "        # TODO: handle cases where is_consp but no label! they should not be brough to 0 0 0 0\n",
    "        \n",
    "        # cases that should be covered by MSE (non-consp + consp where gt is known)\n",
    "        mask = (y_true[:,0] == 0)|((y_true[:,0] == 1)&(y_true[:,1] != -1))\n",
    "        gt_cat = torch.zeros((mask.sum(), y_cat_pred.shape[1]), device=y_cat_pred.device)\n",
    "        # gt_cat[y_true[:,1]!=-1, y_true[y_true[:,1]!=-1,1].long()] = 1.\n",
    "    \n",
    "        gt_cat[:, y_true[mask,1].long()] = 1.\n",
    "        loss_mse = mse(torch.sigmoid(y_bin_pred[mask]).reshape(-1,1) * y_cat_pred[mask], gt_cat)\n",
    "        \n",
    "        l_bin = 1.\n",
    "        l_cat = 10.\n",
    "        l_mse = 0.\n",
    "        \n",
    "        return l_bin * loss_bin + l_cat * torch.nan_to_num(loss_xent) + l_mse * torch.nan_to_num(loss_mse)\n",
    "    return loss_fct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f2698dcf-d7b3-40a7-aa1e-b8a3250857a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens 15393\n"
     ]
    }
   ],
   "source": [
    "print(\"n_tokens\", n_tokens)\n",
    "model = ConvModel(n_tokens).cuda()\n",
    "opt = optim.Adam(model.parameters())\n",
    "# loss_fct = nn.BCEWithLogitsLoss()\n",
    "loss_fct = bce_xent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ed274d20-f0fc-4ef0-89c5-96fd62fee554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "ds = TensorDataset(X_train, y_train)\n",
    "dl = DataLoader(ds, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "df2a747d-0887-4932-a9dd-300aaa2294d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ef515471-0d4f-4aa0-927c-0301639683e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d498a571-c295-410e-8d81-50235194bcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:04<00:00, 82.11it/s, loss=9.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11657225 A 0.650877149450386 B 0.2892279241793805 TOTAL 0.5062174593419837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:03<00:00, 113.42it/s, loss=7.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0247553 A 0.6756454816285998 B 0.35978270549470115 TOTAL 0.5493003711750404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:03<00:00, 117.65it/s, loss=5.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.073003605 A 0.6626249467645364 B 0.3812636165577342 TOTAL 0.5500804146818156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:03<00:00, 118.90it/s, loss=4.93] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17478035 A 0.663260177584185 B 0.4448078380164756 TOTAL 0.5758792417571013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:03<00:00, 116.26it/s, loss=3.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0042180717 A 0.6606491874214466 B 0.4542123351668441 TOTAL 0.5780744465196056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:03<00:00, 118.46it/s, loss=1.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.17772073 A 0.6737882119738594 B 0.47413793103448276 TOTAL 0.5939280995981088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:03<00:00, 118.34it/s, loss=2]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.21670625 A 0.6725206024438761 B 0.4392937399678972 TOTAL 0.5792298574534847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:03<00:00, 118.42it/s, loss=4.53] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.018782407 A 0.6689063245047466 B 0.4061156103839031 TOTAL 0.5637900388564092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:03<00:00, 117.22it/s, loss=1.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.053276807 A 0.6775978024059865 B 0.4062424982366989 TOTAL 0.5690556807382715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:03<00:00, 124.18it/s, loss=0.722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.30573705 A 0.6979066022544284 B 0.39814535868951234 TOTAL 0.5780021048284619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:02<00:00, 140.47it/s, loss=0.475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.15979928 A 0.6916928026736668 B 0.4409523809523809 TOTAL 0.5913966339851524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:02<00:00, 140.48it/s, loss=0.773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2957174 A 0.691122730083769 B 0.381986129234933 TOTAL 0.5674680897442346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:02<00:00, 134.71it/s, loss=0.545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3974994 A 0.7053083923154702 B 0.4183582287006301 TOTAL 0.5905283268695342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:03<00:00, 118.05it/s, loss=0.585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4745493 A 0.6998776624109916 B 0.37292216996327565 TOTAL 0.5690954654319051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:03<00:00, 111.84it/s, loss=0.799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.06326407 A 0.6878021895433024 B 0.43505661005661006 TOTAL 0.5867039577486255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:03<00:00, 118.57it/s, loss=0.381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2542718 A 0.6810779408182005 B 0.4393305693390945 TOTAL 0.5843789922265581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:03<00:00, 117.84it/s, loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40123576 A 0.697532102728732 B 0.434993465653843 TOTAL 0.5925166478987764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:03<00:00, 117.95it/s, loss=0.549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06403023 A 0.710372793175341 B 0.40376075401391853 TOTAL 0.587727977510772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:03<00:00, 118.30it/s, loss=0.228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3759104 A 0.6987794380832657 B 0.4261083743842365 TOTAL 0.589711012603654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:03<00:00, 118.20it/s, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.272348 A 0.718996190056362 B 0.4510193069110266 TOTAL 0.6118054367982277\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cumul_loss = 0\n",
    "\n",
    "y_val_bin = y_val.cpu().detach().numpy()[:, 0]\n",
    "y_val_cat = y_val.cpu().detach().numpy()[:, 1]\n",
    "\n",
    "for epoch in range(20):\n",
    "    with tqdm(dl) as bar:\n",
    "        for X_batch, y_batch in bar:\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            y_pred_bin, y_pred_cat = model(X_batch)\n",
    "            \n",
    "            loss = loss_fct(y_pred_bin, y_pred_cat, y_batch)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            if np.isnan(loss.item()):\n",
    "                break\n",
    "            cumul_loss = cumul_loss * .7 + loss.item() * .3\n",
    "            bar.set_postfix(loss=cumul_loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            y_pred_val_bin, y_pred_val_cat  = model(X_val)\n",
    "            y_pred_val_bin = y_pred_val_bin.cpu().detach().numpy()\n",
    "            y_pred_val_cat = y_pred_val_cat.cpu().detach().numpy().argmax(axis=1)\n",
    "\n",
    "            \n",
    "            fpr, tpr, thresh = roc_curve(y_val_bin, y_pred_val_bin)\n",
    "            best_thresh = thresh[((1-tpr)**2 + fpr**2).argmin()]\n",
    "            y_pred_val_bin = y_pred_val_bin > best_thresh\n",
    "            \n",
    "            mask = y_val_cat != -1\n",
    "            \n",
    "            f1_bin = f1_score(y_val_bin, y_pred_val_bin, average=\"macro\")\n",
    "            f1_cat = f1_score(y_val_cat[mask], y_pred_val_cat[mask], average=\"macro\")\n",
    "            \n",
    "            print(best_thresh,\"A\",f1_bin, \"B\", f1_cat, \"TOTAL\", f1_bin * .6 + f1_cat * .4)\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c4c0cf4-83f6-4ee3-9c86-acee23ec78f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e13243b-7bbd-4ab9-8534-d4957037246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3fe6fd-8117-45fb-9011-be0c8e272b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c06647-cb51-44d3-a2f7-a1d1b3cef79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7859071514380385"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6c7d86-2132-4c16-9c5c-2f375d7785a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8548, -1.3396,  0.2361, -0.7347, -1.3120, -0.7089,  0.3607, -1.3095,\n",
       "        -1.1451,  0.3435,  0.0540, -0.5253,  0.0278, -0.1222,  0.2268,  0.0471,\n",
       "        -1.0794,  0.0023,  0.5623, -1.2321,  0.0393, -0.3050, -0.7990, -0.5041,\n",
       "         0.1838,  0.6330, -1.1760, -0.3897,  0.4732, -0.1427, -0.2376, -0.2320,\n",
       "         0.6403, -0.0478, -1.1678,  0.1183,  0.5759, -0.4073,  0.3987,  0.6902,\n",
       "        -1.2335,  0.2996,  0.1060, -1.1795, -0.4005, -0.4899,  0.0402, -1.1941,\n",
       "        -0.3558, -0.3094,  0.2454,  0.5827,  0.3673, -1.3603,  0.2070, -1.2239,\n",
       "        -1.1214,  0.1465,  0.4019,  0.1645, -0.3064, -1.1178, -0.6988,  0.4551,\n",
       "        -0.5320, -0.4539,  0.3870,  0.2336,  0.0815,  0.4184,  0.3905,  0.0836,\n",
       "        -0.1238, -1.3310, -0.7574, -0.2628, -0.3751, -1.2006, -0.0078,  0.4473,\n",
       "        -0.1176, -0.0770,  0.3524,  0.0364,  0.4665, -0.2952, -0.1799,  0.2434,\n",
       "         0.4023, -0.0513,  0.6520,  0.4968,  0.1165,  0.4282, -0.3582, -1.3414,\n",
       "         0.5521,  0.3326,  0.2808,  0.3612,  0.1189, -1.3487, -0.1823,  0.0877,\n",
       "         0.1349, -1.1756,  0.0841, -0.6034,  0.5790, -1.2286,  0.2349, -0.4462,\n",
       "        -1.1715,  0.1571, -0.0818,  0.0014, -0.3622, -0.0644, -0.2246,  0.7142,\n",
       "         0.1876, -0.9047,  0.1292,  0.6250, -0.6597, -1.3319,  0.4008,  0.3236,\n",
       "        -0.0807,  0.6905, -0.5232, -0.0048,  0.7976, -0.2780,  0.5849, -1.3193,\n",
       "        -0.8265,  0.4732, -1.0818, -0.1331, -1.1611, -0.0487, -0.3157, -1.2658,\n",
       "         0.5022, -1.1941, -1.0968,  0.5545,  0.2404, -0.8128,  0.4287, -0.4662,\n",
       "         0.3076, -0.1444,  0.2716, -0.0490, -0.1169,  0.1729, -0.1887,  0.3307,\n",
       "        -0.2087,  0.0779, -1.2308, -0.1864,  0.1473,  0.0110, -0.9640,  0.4716,\n",
       "        -0.9458,  0.5081, -0.0334, -0.1855,  0.5693,  0.1776,  0.0430,  0.1953,\n",
       "        -0.9202, -0.6561,  0.0442, -0.0328, -0.4788, -0.3353, -0.3905, -1.1668,\n",
       "         0.4452,  0.2338, -0.1077,  0.0283, -0.9715,  0.8397, -0.4021, -1.1460,\n",
       "         0.0977,  0.1756, -0.3135,  0.1104, -1.2247, -0.1114, -0.1543, -0.1036,\n",
       "         0.6272,  0.1066, -0.3750,  0.6438,  0.0016,  0.2996, -0.4142, -0.6954,\n",
       "        -0.3936, -1.1640, -0.8451,  0.0390,  0.7737, -0.0285,  0.0361, -0.2309,\n",
       "        -1.1919,  0.5704,  0.3322,  0.5123, -0.4738,  0.2772,  0.3302,  0.4827,\n",
       "         0.0735,  0.1080, -0.4081, -0.2952,  0.2971, -0.3061,  0.9249,  0.2173,\n",
       "        -1.3181,  0.2696,  0.6453, -0.2067, -0.0285, -0.1241,  0.0954, -0.1438,\n",
       "         0.1041,  0.3277,  0.2710, -0.2826,  1.0011,  0.0044,  0.5525,  0.6928,\n",
       "         0.8113, -0.4431, -0.7984, -0.2704,  0.2519, -0.2611,  0.2430, -1.2736,\n",
       "         0.4991, -0.3854, -1.2823,  0.3374, -0.0813,  0.2901, -0.2319,  0.4308,\n",
       "        -0.6167,  0.3342, -0.8759, -0.3860, -1.2509,  0.3692,  0.2436, -1.2305,\n",
       "        -0.9087, -0.0126, -1.2957, -0.0755, -0.2526,  0.4017, -0.3867, -0.2398,\n",
       "        -0.7309, -1.2950, -0.0648,  0.4720, -0.8482, -0.4249, -1.1636, -0.3824,\n",
       "        -0.0438,  0.3524,  0.1216,  0.8999, -1.3622,  0.4470,  0.0197, -0.7730,\n",
       "        -0.2936,  0.3898,  0.4598, -0.4472,  0.1880, -0.1938,  0.1767, -0.0191,\n",
       "         0.3948, -0.9497, -1.3075,  0.1999, -0.5470,  0.3074, -1.2810,  0.2002,\n",
       "         0.0921, -0.0415, -1.2072, -0.4110, -0.1808, -0.4348,  0.4206,  0.2738,\n",
       "        -0.2479,  0.1546,  0.1207, -1.2161,  0.3666,  0.1406,  0.6509,  0.1771,\n",
       "        -0.2505,  0.0035, -1.3504, -0.3229, -0.2165, -0.1661, -0.6354,  0.4391,\n",
       "        -0.7951, -0.2411, -0.1525, -0.3563,  0.0975, -0.7883, -0.5325,  0.0552,\n",
       "        -0.3864, -1.2047,  0.0668, -1.1989, -0.0327, -1.0740,  0.5380, -0.0600,\n",
       "        -1.2407, -0.0844,  0.3568, -0.2118, -0.3369, -0.0440, -0.1910, -1.2123,\n",
       "        -0.1484, -0.0821, -0.3622,  0.3906,  0.1407, -0.0564, -0.4700, -0.5551,\n",
       "        -0.3042], device='cuda:0')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_val_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ecf06-fb3e-4046-86a3-6e7fa131e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X = tfidf.fit_transform(df.comment_text)\n",
    "# y = df.conspiratorial.values\n",
    "y = df.conspiracy.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ecba20-db33-462a-9568-4e3568cbca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7085cdd2-e208-4c1b-9bc2-59ad7243451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "664f3ffc-6e4c-4cd6-a1e0-7c5fc044974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      1.00      0.79        90\n",
      "           1       0.73      0.34      0.46        47\n",
      "           2       0.00      0.00      0.00        14\n",
      "           3       1.00      0.18      0.31        11\n",
      "\n",
      "    accuracy                           0.67       162\n",
      "   macro avg       0.59      0.38      0.39       162\n",
      "weighted avg       0.64      0.67      0.59       162\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "227e68ee-3ded-4293-9a1e-9a0b0730a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    (logits, _), labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # predictions = torch.max(logits, axis=1).indices\n",
    "    return f1.compute(predictions=predictions, references=labels, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e2f398ad-9bbe-4a8c-9cd3-332779d276e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fdcfd897-75ef-4e42-a63c-f675d47a427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"subtaskB_train.csv\", index_col=0)\n",
    "df.drop(columns=[\"topic\"], inplace=True)\n",
    "df.columns = [\"text\", \"labels\"]\n",
    "df_train, df_test = train_test_split(df, train_size=.8)\n",
    "\n",
    "ds_train = Dataset.from_pandas(df_train, split=\"train\")\n",
    "ds_test = Dataset.from_pandas(df_test, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0a167f84-1719-40c8-b2df-b40ee9057b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7087bfd5-963d-4de5-908d-bfd46f5656f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"morenolq/bart-it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "757c6588-cc27-4ab5-8f9a-409a5ee94f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/162 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_train_tok = ds_train.map(tokenize_func, batched=True)\n",
    "ds_test_tok = ds_test.map(tokenize_func, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1aac5105-7539-44c7-ad42-debef43e8de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9a5d8b7c-e02f-4564-bf96-a106977079ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "f1 = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e4873b2d-b4f3-42ab-8a23-ce59e42cf265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at morenolq/bart-it were not used when initializing BartForSequenceClassification: ['lm_head.weight', 'final_logits_bias']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at morenolq/bart-it and are newly initialized: ['classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.out_proj.bias', 'classification_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"morenolq/bart-it\", num_labels=4)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"morenolq/bart-it\")\n",
    "# oppure, dipende da quello che volete fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0d7bcf60-0c10-4438-b89e-b56e3145add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args, \n",
    "    train_dataset=ds_train_tok,\n",
    "    eval_dataset=ds_test_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0b977942-6793-4fd2-a008-0452aa3fe1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_params(model):\n",
    "    return sum([ np.prod(tuple(x.shape)) for x in list(model.parameters()) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "03155df6-927d-4c69-bc54-8aec6ce5a6e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141346564"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "aacd497e-387a-4b86-9948-0838eb03c1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='405' max='405' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [405/405 00:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.500953</td>\n",
       "      <td>0.784207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.468956</td>\n",
       "      <td>0.850243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.681765</td>\n",
       "      <td>0.839796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.670220</td>\n",
       "      <td>0.858380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.697096</td>\n",
       "      <td>0.852897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=405, training_loss=0.1985308235074267, metrics={'train_runtime': 58.6632, 'train_samples_per_second': 55.231, 'train_steps_per_second': 6.904, 'total_flos': 388157033664000.0, 'train_loss': 0.1985308235074267, 'epoch': 5.0})"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de51764b-5c33-4ed4-b0db-7ec0d7421c44",
   "metadata": {},
   "source": [
    "## Multi-Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e2935ac0-698e-49bc-a9bf-2a869da6cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0dea0bd6-cb40-40f5-aa7e-d566563575f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import ModelOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b63ec10d-9d6a-493c-80ea-02ffc87eef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, encoder, max_size=200, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc1 = nn.Linear(768, 768)\n",
    "        if n_classes == 2:\n",
    "            self.fc2 = nn.Linear(768, 1)\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        else:\n",
    "            self.fc2 = nn.Linear(768, n_classes)\n",
    "            self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.encoder._init_weights(self.fc1)\n",
    "        self.encoder._init_weights(self.fc2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        x = self.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        x = x[:, 0] # use only [CLS]\n",
    "        x = self.fc1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        if labels is not None:\n",
    "            loss = self.loss(x, labels)\n",
    "            return ModelOutput({\"loss\": loss, \"logits\": x })\n",
    "\n",
    "        return ModelOutput({ \"logits\": x })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9185e36a-a74d-467e-8040-72dd1339138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import ModelOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b6f05a15-49eb-46e7-b782-634ca99d9194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class MultiTaskTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        # self.loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # implement custom logic here\n",
    "        output = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "        \n",
    "        loss = self.loss(output.get(\"logits\"), inputs[\"labels\"])\n",
    "        if return_outputs:\n",
    "            return loss, output\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1b7fe4fe-3477-43d1-a659-36895295c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e9ef4abf-60a6-42f7-964e-7bd1945b67a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at morenolq/bart-it were not used when initializing BartModel: ['lm_head.weight', 'final_logits_bias']\n",
      "- This IS expected if you are initializing BartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "encoder = AutoModel.from_pretrained(\"morenolq/bart-it\")#, output_hidden_states=True)\n",
    "model = MultiTaskModel(encoder=encoder, n_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "99c7c298-7188-4102-8c80-05b39589295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=10,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    # (logits, _), labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # predictions = torch.max(logits, axis=1).indices\n",
    "    return f1.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "trainer = Trainer(\n",
    "# trainer = MultiTaskTrainer(\n",
    "    model=model,\n",
    "    args=training_args, \n",
    "    train_dataset=ds_train_tok,\n",
    "    eval_dataset=ds_test_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "950c6765-0017-4259-8436-aa1295a0a751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='810' max='810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [810/810 01:58, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.540235</td>\n",
       "      <td>0.782136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.563055</td>\n",
       "      <td>0.800301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.708294</td>\n",
       "      <td>0.831806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.801645</td>\n",
       "      <td>0.867346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.869802</td>\n",
       "      <td>0.862512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.905962</td>\n",
       "      <td>0.872841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>0.921589</td>\n",
       "      <td>0.872841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>0.931924</td>\n",
       "      <td>0.871923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>0.937373</td>\n",
       "      <td>0.871923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>0.939788</td>\n",
       "      <td>0.871923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=810, training_loss=0.1260279884170971, metrics={'train_runtime': 118.3858, 'train_samples_per_second': 54.736, 'train_steps_per_second': 6.842, 'total_flos': 0.0, 'train_loss': 0.1260279884170971, 'epoch': 10.0})"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
